library(odbc)
library(DBI)
library(dplyr)
library(tidyverse)
library(tidyr)
library(hash)
library(glmnet)
library(Metrics)
library(MetricsWeighted)
library(caret)
library(pROC)
library(vtreat)
library(randomForest)
colSums(is.na(train))
colSums(is.na(non_numerical_train)) # Already_Defaulted has 26 missing values: i will drop it
train <- (train[complete.cases(train),])
for (i in list(keys(dic))) {
k = values(dic[i])[[1]]
if (k ==  'median') {
train[i][is.na(train[i])] <- median(
as.numeric(unlist(train[i])), na.rm=T)
}
else {
train[i][is.na(train[i])] <- mean(
as.numeric(unlist(train[i])), na.rm=T)
}
}
library(odbc)
library(DBI)
library(dplyr)
library(tidyverse)
library(tidyr)
library(hash)
library(glmnet)
library(Metrics)
library(MetricsWeighted)
library(caret)
library(pROC)
# connect to the database using odbc
con <- dbConnect(odbc::odbc(),
Driver = "SQL Server",
Server = "LOGISTICS\\SQLEXPRESS",
Database = "LoanDefault",
Port = 1433)
train_tbl <- tbl(con, 'Data_Train')
test_tbl <- tbl(con, 'Data_Test')
train <- collect(train_tbl)
test <- collect(test_tbl)
colSums(is.na(train)) # there is no missing values in any of the column
colSums(is.na(test))
# check for duplicate in train and test data: no duplicate data
sum(duplicated(train)) # no duplicate
sum(duplicated(test))
numerical_train <- train %>% select_if(is.numeric)
non_numerical_train <- train %>% select_if(negate(is.numeric))
train_tbl <- tbl(con, 'Data_Train')
test_tbl <- tbl(con, 'Data_Test')
train <- collect(train_tbl)
test <- collect(test_tbl)
# CHECK FOR MISSING VALUES IN THE DATASET
colSums(is.na(train)) # there is no missing values in any of the column
colSums(is.na(test))
sum(duplicated(train)) # no duplicate
sum(duplicated(test))
# obtain the numerical column and non numerical column: train data
numerical_train <- train %>% select_if(is.numeric)
non_numerical_train <- train %>% select_if(negate(is.numeric))
dim(numerical_train) # rows:columns <> 87500:18
dim(non_numerical_train)
paste("% of FALSE values in target variables:",
as.character(round((sum(train$Loan_No_Loan == FALSE) / nrow(train) * 100), 2))) # 81.13%
paste("% of TRUE values in target variables:",
as.character(round((sum(train$Loan_No_Loan == TRUE) / nrow(train) * 100), 2))) # 18.87%
miss_num <- as.data.frame(as.matrix(colSums(is.na(numerical_train))))
miss_num <- rename(miss_num, no_missing_values = V1)
miss_num$variables <- rownames(miss_num)
rownames(miss_num) <- NULL
miss_num
miss_num_names <- miss_num[miss_num['no_missing_values'] > 0, ]$variables
miss_num_names
hist(numerical_train$Yearly_Income) # median
dic <- hash(keys=miss_num_names[-3], values= c('median', 'mean', 'median', 'median'))
dic <- hash(keys=miss_num_names[-2], values= c('median', 'mean', 'median', 'median'))
for (i in list(keys(dic))) {
k = values(dic[i])[[1]]
if (k ==  'median') {
train[i][is.na(train[i])] <- median(
as.numeric(unlist(train[i])), na.rm=T)
}
else {
train[i][is.na(train[i])] <- mean(
as.numeric(unlist(train[i])), na.rm=T)
}
}
colSums(is.na(train))
colSums(is.na(non_numerical_train)) # Already_Defaulted has 26 missing values: i will drop it
train <- (train[complete.cases(train),])
colSums(is.na(train))
new_numeric <- train[, names(numerical_train)]
new_numeric['Loan_No_Loan'] <- train$Loan_No_Loan
new_numeric$Loan_No_Loan <- as.numeric(new_numeric$Loan_No_Loan)
new_numeric$Loan_No_Loan
names(non_numerical_train)
ordinal_var <- c("GGGrade", "Experience", "Already_Defaulted", "Duration")
sapply(lapply(train[, ordinal_var], unique), length)
unique(non_numerical_train['Experience'])
gggrade <- hash(c('I', 'II', 'III', 'IV', 'V', 'VI', 'VII'), c(1,2,3,4,5,6,7))
duration <- hash(c('3 years', '5 years'), c(1,2))
experience <- hash(c('<1yr', '1yrs', '2yrs', '3yrs', '4yrs', '5yrs', '6yrs',
'7yrs', '8yrs', '9yrs', '>10yrs'),
c(1,2,3,4,5,6,7,8,9,10,11))
for (k in keys(gggrade)) {
train['GGGrade'][train['GGGrade'] == k] <- as.character(values(gggrade[k])[[1]])
}
new_numeric['GGGrade'] <- as.numeric(train$GGGrade)
for (k in keys(duration)) {
train['Duration'][train['Duration'] == k] <- as.character(values(duration[k])[[1]])
}
new_numeric['Duration'] <- as.numeric(train$Duration)
for (k in keys(experience)) {
train['Experience'][train['Experience'] == k] <- as.character(values(experience[k])[[1]])
}
new_numeric['Experience'] <- as.numeric(train$Experience)
set.seed(1)
gp <- runif(nrow(new_numeric))
train.subset <- new_numeric[gp<=0.8, ][-1]
test.subset <- new_numeric[gp>0.8, ]
# test label
y_test <- new_numeric[gp>0.8, ]$Loan_No_Loan
length(y_test)
y_test
dim(train.subset)
dim(test.subset)
log.model <- glm(Loan_No_Loan ~., data=train.subset, family='binomial')
log.model.prob <- predict(log.model, test.subset, type='response')
log.model.prob[1:10]
length(log.model.prob)
log.model.pred <- ifelse(log.model.prob > 0.50, 1, 0)
log.model.pred[1:10]
y_test[1:10]
length(y_test)
length(log.model.pred)
confussion_mat <- confusionMatrix(data = factor(log.model.pred), reference = factor(y_test))
confussion_mat
null_model <-  glm(Loan_No_Loan ~ 1, data=train.subset, family='binomial')
full_model <-  glm(Loan_No_Loan ~., data=train.subset, family='binomial')
step_model <- step(null_model, scope=list(lower=null_model, upper=full_model), direction='forward')
important_features <- names(coef(step_model))[-1]
important_features
important_features
new_df <- train[, important_features]
new_df['Loan_No_Loan'] <- as.numeric(train$Loan_No_Loan)
names(new_df)
set.seed(1)
gp <- runif(nrow(new_df))
# split the data into 20% test and 80% train
train.subset <- new_df[gp<=0.8, ]
test.subset <- new_df[gp>0.8, ]
y_test <- new_df[gp>0.8, ]$Loan_No_Loan
length(y_test)
log.model <- glm(Loan_No_Loan ~., data=train.subset, family='binomial')
# predict on the test data
log.model.prob <- predict(log.model, test.subset, type='response')
log.model.prob[1:10]
length(log.model.prob)
log.model.pred <- ifelse(log.model.prob > 0.50, 1, 0)
log.model.pred[1:10]
y_test[1:10]
length(y_test)
length(log.model.pred)
confussion_mat <- confusionMatrix(data = factor(log.model.pred), reference = factor(y_test))
confussion_mat
new_newer_train <- train[, important_features]
names(new_newer_train)
new_newer_train$Loan_No_Loan <- train$Loan_No_Loan
# create a factor for the target variable
new_newer_train$Loan_No_Loan <- factor(new_newer_train$Loan_No_Loan,
levels = c('FALSE', 'TRUE'),
labels =  c(0,1))
log.model <- glm(Loan_No_Loan ~., data=new_newer_train, family='binomial')
new_test <- test[, important_features]
colSums(is.na(new_test))
summary(train$Unpaid_Amount) # use median
summary(train$Yearly_Income) # use median
new_test %>% group_by(Postal_Code) %>% summarise(count_=length(Postal_Code)) # highest: 1000
new_test$Unpaid_Amount[is.na(new_test$Unpaid_Amount)] <- median(train$Unpaid_Amount, na.rm=T)
colSums(is.na(new_test))
View(new_test)
for (k in keys(duration)) {
new_test['Duration'][new_test['Duration'] == k] <- as.character(values(duration[k])[[1]])
}
new_test['Duration'] <- as.factor(new_test$Duration)
log.model.prob <- predict(log.model, new_test, type='response')
pred_test_table <- data.frame(S.No=c(1:nrow(test)), Id=test$ID, prediction=log.model.pred)
log.model.pred <- ifelse(log.model.prob > 0.50, 1, 0)
pred_test_table <- data.frame(S.No=c(1:nrow(test)), Id=test$ID, prediction=log.model.pred)
unique(pred_test_table$prediction)
pred_test_table <- data.frame(S.No=c(1:nrow(test)), Id=test$ID, prediction=log.model.pred)
write.csv(pred_test_table, "PredictedLoanDefault.csv", row.names=FALSE)
